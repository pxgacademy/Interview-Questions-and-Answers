# 18. Sitemap, Robots.txt, Canonical URLs

**সাইটম্যাপ (Sitemap)**, **Robots.txt**, এবং **ক্যানোনিকাল URLs** হল ওয়েবসাইটের **SEO (Search Engine Optimization)** এবং ক্রলিং প্রক্রিয়া উন্নত করার জন্য গুরুত্বপূর্ণ টুল। এগুলো সার্চ ইঞ্জিনকে ওয়েবসাইটের কাঠামো বুঝতে, কোন পেজ ক্রল করতে হবে তা নির্ধারণ করতে, এবং ডুপ্লিকেট কনটেন্ট এড়াতে সহায়তা করে। Next.js অ্যাপ্লিকেশনে এই উপাদানগুলো সঠিকভাবে বাস্তবায়ন করা সার্চ ইঞ্জিনে দৃশ্যমানতা বাড়ানো এবং ইউজার এক্সপেরিয়েন্স উন্নত করার জন্য অত্যন্ত গুরুত্বপূর্ণ। সিনিয়র ডেভেলপারদের জন্য এই কৌশলগুলো সঠিকভাবে প্রয়োগ করা SEO, পারফরম্যান্স, এবং ওয়েবসাইট ম্যানেজমেন্টে গুরুত্বপূর্ণ ভূমিকা পালন করে। এই বিস্তারিত আলোচনায় আমরা Next.js-এর প্রেক্ষাপটে সাইটম্যাপ, Robots.txt, এবং ক্যানোনিকাল URLs নিয়ে বাংলায় আলোচনা করব।

---

## সাইটম্যাপ, Robots.txt, এবং ক্যানোনিকাল URLs কী?

1. **সাইটম্যাপ (Sitemap)**:
   - সাইটম্যাপ হল একটি ফাইল (সাধারণত XML ফরম্যাটে), যা ওয়েবসাইটের সব পেজের তালিকা এবং তাদের গুরুত্ব সম্পর্কে তথ্য প্রদান করে।
   - উদ্দেশ্য:
     - সার্চ ইঞ্জিনকে (যেমন Google, Bing) ওয়েবসাইটের পেজ ক্রল করতে সহায়তা করা।
     - পেজের আপডেট ফ্রিকোয়েন্সি এবং অগ্রাধিকার (priority) নির্দেশ করা।
   - প্রকার:
     - **XML Sitemap**: সার্চ ইঞ্জিনের জন্য।
     - **HTML Sitemap**: ইউজারদের জন্য নেভিগেশন সহায়তা।
   - উদাহরণ:
     ```xml
     <?xml version="1.0" encoding="UTF-8"?>
     <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
       <url>
         <loc>https://example.com</loc>
         <lastmod>2023-10-01</lastmod>
         <changefreq>monthly</changefreq>
         <priority>1.0</priority>
       </url>
     </urlset>
     ```

2. **Robots.txt**:
   - Robots.txt হল একটি টেক্সট ফাইল, যা ওয়েবসাইটের রুট ডিরেক্টরিতে থাকে এবং সার্চ ইঞ্জিন ক্রলারদের কোন পেজ ক্রল করতে হবে বা করতে হবে না তা নির্দেশ করে।
   - উদ্দেশ্য:
     - অপ্রয়োজনীয় পেজ (যেমন অ্যাডমিন পেজ) ক্রলিং বন্ধ করা।
     - ক্রলিং বাজেট অপটিমাইজ করা।
   - উদাহরণ:
     ```txt
     User-agent: *
     Disallow: /admin/
     Allow: /
     Sitemap: https://example.com/sitemap.xml
     ```

3. **ক্যানোনিকাল URLs**:
   - ক্যানোনিকাল URLs হল `<link rel="canonical">` ট্যাগ, যা সার্চ ইঞ্জিনকে জানায় যে একটি পেজের মূল (preferred) URL কোনটি।
   - উদ্দেশ্য:
     - ডুপ্লিকেট কনটেন্ট সমস্যা সমাধান করা।
     - SEO-তে পেজের র‍্যাঙ্কিং উন্নত করা।
   - উদাহরণ:
     ```html
     <link rel="canonical" href="https://example.com/product/123" />
     ```

---

## Next.js-এ সাইটম্যাপ, Robots.txt, এবং ক্যানোনিকাল URLs

Next.js-এ এই উপাদানগুলো স্ট্যাটিক এবং ডায়নামিকভাবে বাস্তবায়ন করা যায়। App Router এবং Pages Router উভয়ই এই কৌশল সমর্থন করে।

### ১. সাইটম্যাপ (Sitemap)

Next.js-এ সাইটম্যাপ স্ট্যাটিক বা ডায়নামিকভাবে তৈরি করা যায়। ডায়নামিক সাইটম্যাপের জন্য API বা ডাটাবেস থেকে পেজ তালিকা ফেচ করা হয়।

#### স্ট্যাটিক সাইটম্যাপ:

```javascript
// public/sitemap.xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://example.com</loc>
    <lastmod>2023-10-01</lastmod>
    <changefreq>monthly</changefreq>
    <priority>1.0</priority>
  </url>
  <url>
    <loc>https://example.com/about</loc>
    <lastmod>2023-10-01</lastmod>
    <changefreq>yearly</changefreq>
    <priority>0.8</priority>
  </url>
</urlset>
```

#### ডায়নামিক সাইটম্যাপ (Pages Router):

```javascript
// pages/api/sitemap.xml.js
import { SitemapStream, streamToPromise } from "sitemap";
import { Readable } from "stream";

export default async function handler(req, res) {
  res.setHeader("Content-Type", "text/xml");

  const sitemap = new SitemapStream({ hostname: "https://example.com" });
  const pages = [
    { url: "/", changefreq: "daily", priority: 1.0 },
    { url: "/about", changefreq: "monthly", priority: 0.8 },
  ];

  // API থেকে ডায়নামিক পেজ ফেচ
  const posts = await fetchPosts(); // ধরা যাক এটি পোস্ট তালিকা ফেরত দেয়
  posts.forEach((post) => {
    pages.push({
      url: `/post/${post.id}`,
      changefreq: "weekly",
      priority: 0.7,
      lastmod: post.updatedAt,
    });
  });

  const stream = Readable.from(pages).pipe(sitemap);
  const xml = await streamToPromise(stream);

  res.write(xml.toString());
  res.end();
}
```

#### ডায়নামিক সাইটম্যাপ (App Router):

```javascript
// app/sitemap.js
import { fetchPosts } from "@/lib/api";

export default async function sitemap() {
  const posts = await fetchPosts(); // API থেকে পোস্ট ফেচ
  const staticPages = [
    {
      url: "https://example.com",
      lastModified: new Date(),
      changeFrequency: "daily",
      priority: 1.0,
    },
    {
      url: "https://example.com/about",
      lastModified: new Date(),
      changeFrequency: "monthly",
      priority: 0.8,
    },
  ];

  const dynamicPages = posts.map((post) => ({
    url: `https://example.com/post/${post.id}`,
    lastModified: new Date(post.updatedAt),
    changeFrequency: "weekly",
    priority: 0.7,
  }));

  return [...staticPages, ...dynamicPages];
}
```

#### সাইটম্যাপ সাবমিশন:
- Google Search Console-এ সাইটম্যাপ সাবমিট করুন: `https://example.com/sitemap.xml`।
- Robots.txt-এ সাইটম্যাপের লিঙ্ক যোগ করুন:
  ```txt
  Sitemap: https://example.com/sitemap.xml
  ```

### ২. Robots.txt

Robots.txt ফাইল ওয়েবসাইটের রুটে (`public/robots.txt`) রাখা হয়। Next.js-এ এটি স্ট্যাটিক বা ডায়নামিকভাবে তৈরি করা যায়।

#### স্ট্যাটিক Robots.txt:

```txt
# public/robots.txt
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /
Sitemap: https://example.com/sitemap.xml
```

#### ডায়নামিক Robots.txt:

```javascript
// pages/api/robots.txt.js
export default function handler(req, res) {
  res.setHeader("Content-Type", "text/plain");
  res.write(`
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /
Sitemap: https://example.com/sitemap.xml
  `);
  res.end();
}
```

#### App Router-এ Robots.txt:

```javascript
// app/robots.js
export default function robots() {
  return {
    rules: [
      {
        userAgent: "*",
        allow: ["/"],
        disallow: ["/admin/", "/private/"],
      },
    ],
    sitemap: "https://example.com/sitemap.xml",
  };
}
```

### ৩. ক্যানোনিকাল URLs

ক্যানোনিকাল URLs `<Head>` কম্পোনেন্ট (Pages Router) বা `metadata` API (App Router) ব্যবহার করে সেট করা হয়।

#### Pages Router-এ ক্যানোনিকাল URLs:

```jsx
// pages/post/[id].js
import Head from "next/head";
import { useRouter } from "next/router";

export async function getServerSideProps({ params }) {
  const post = await fetchPostById(params.id);
  return { props: { post } };
}

export default function PostPage({ post }) {
  const router = useRouter();
  return (
    <div>
      <Head>
        <title>{post.title}</title>
        <link rel="canonical" href={`https://example.com/post/${router.query.id}`} />
      </Head>
      <h1>{post.title}</h1>
    </div>
  );
}
```

#### App Router-এ ক্যানোনিকাল URLs:

```jsx
// app/post/[id]/page.js
import { fetchPostById } from "@/lib/api";

export async function generateMetadata({ params }) {
  const post = await fetchPostById(params.id);
  return {
    title: post.title,
    alternates: {
      canonical: `https://example.com/post/${params.id}`,
    },
  };
}

export default async function PostPage({ params }) {
  const post = await fetchPostById(params.id);
  return (
    <div>
      <h1>{post.title}</h1>
    </div>
  );
}
```

---

## ব্যবহারের কৌশল

1. **সাইটম্যাপ**:
   - **স্ট্যাটিক সাইটম্যাপ**: ছোট ওয়েবসাইটের জন্য `public/sitemap.xml` ব্যবহার।
   - **ডায়নামিক সাইটম্যাপ**: ব্লগ বা ই-কমার্স সাইটের জন্য API বা ডাটাবেস থেকে পেজ তালিকা তৈরি।
   - `lastmod`, `changefreq`, এবং `priority` সঠিকভাবে সেট করুন।
     ```xml
     <url>
       <loc>https://example.com/post/123</loc>
       <lastmod>2023-10-01</lastmod>
       <changefreq>weekly</changefreq>
       <priority>0.7</priority>
     </url>
     ```

2. **Robots.txt**:
   - অপ্রয়োজনীয় পেজ (যেমন `/admin`) ব্লক করতে `Disallow` ব্যবহার করুন।
   - সাইটম্যাপের লিঙ্ক যোগ করুন।
   - ক্রলার-নির্দিষ্ট নিয়ম সেট করুন:
     ```txt
     User-agent: Googlebot
     Disallow: /test/
     ```

3. **ক্যানোনিকাল URLs**:
   - ডুপ্লিকেট পেজ (যেমন `/product/123` এবং `/product/123?sort=price`) এড়াতে ক্যানোনিকাল ট্যাগ সেট করুন।
   - মাল্টি-লিঙ্গুয়াল সাইটে `hreflang` এবং ক্যানোনিকাল ট্যাগ একসাথে ব্যবহার করুন:
     ```jsx
     <link rel="canonical" href="https://example.com/en/product/123" />
     <link rel="alternate" hrefLang="bn" href="https://example.com/bn/product/123" />
     ```

---

## ব্যবহারের ক্ষেত্র

1. **ই-কমার্স**:
   - **সাইটম্যাপ**: প্রতিটি পণ্য পেজের জন্য ডায়নামিক সাইটম্যাপ।
   - **Robots.txt**: অ্যাডমিন এবং কার্ট পেজ ব্লক করা।
   - **ক্যানোনিকাল URLs**: পণ্য পেজের ডুপ্লিকেট URL (যেমন ফিল্টার সহ) এড়ানো।
2. **ব্লগ**:
   - **সাইটম্যাপ**: প্রতিটি পোস্টের জন্য সাইটম্যাপ এন্ট্রি।
   - **Robots.txt**: ড্রাফ্ট পোস্ট বা ট্যাগ পেজ ব্লক করা।
   - **ক্যানোনিকাল URLs**: পোস্টের মূল URL নির্দেশ করা।
3. **মাল্টি-লিঙ্গুয়াল ওয়েবসাইট**:
   - **সাইটম্যাপ**: প্রতিটি ভাষার জন্য পেজ তালিকা।
   - **Robots.txt**: ভাষা-নির্দিষ্ট পেজ ক্রল করার নিয়ম।
   - **ক্যানোনিকাল URLs**: ভাষা-নির্দিষ্ট ক্যানোনিকাল ট্যাগ।
4. **এন্টারপ্রাইজ অ্যাপ**:
   - **সাইটম্যাপ**: ড্যাশবোর্ড এবং পাবলিক পেজের তালিকা।
   - **Robots.txt**: প্রাইভেট ড্যাশবোর্ড ব্লক করা।
   - **ক্যানোনিকাল URLs**: ডায়নামিক ড্যাশবোর্ড পেজে ক্যানোনিকাল ট্যাগ।

---

## সিনিয়র ডেভেলপারদের জন্য গুরুত্বপূর্ণ টিপস

1. **SEO অপটিমাইজেশন**:
   - সাইটম্যাপে `lastmod` এবং `priority` সঠিকভাবে সেট করুন।
   - Robots.txt-এ অপ্রয়োজনীয় পেজ ব্লক করে ক্রলিং বাজেট সংরক্ষণ করুন।
   - ক্যানোনিকাল ট্যাগ দিয়ে ডুপ্লিকেট কনটেন্ট এড়ান।
     ```jsx
     <link rel="canonical" href="https://example.com/product/123" />
     ```

2. **পারফরম্যান্স**:
   - ডায়নামিক সাইটম্যাপের জন্য ক্যাশিং ব্যবহার করুন।
     ```javascript
     export const revalidate = 3600; // ১ ঘণ্টার জন্য ক্যাশ
     ```
   - Robots.txt ফাইল ছোট এবং সিম্পল রাখুন।
   - ক্যানোনিকাল ট্যাগ সার্ভার-সাইডে রেন্ডার করুন।

3. **এরর হ্যান্ডলিং**:
   - সাইটম্যাপ তৈরির সময় API ফেইল হলে ফলব্যাক তালিকা ব্যবহার করুন:
     ```javascript
     const pages = posts.length ? posts.map((post) => ({ url: `/post/${post.id}` })) : [{ url: "/" }];
     ```
   - Robots.txt-এ ভুল নিয়ম এড়ান, যা পুরো সাইট ব্লক করতে পারে।
   - ক্যানোনিকাল URL ভুল হলে 404 এরর চেক করুন।

4. **মাল্টি-লিঙ্গুয়াল সাপোর্ট**:
   - সাইটম্যাপে ভাষা-নির্দিষ্ট পেজ যোগ করুন:
     ```xml
     <url>
       <loc>https://example.com/bn/post/123</loc>
       <xhtml:link rel="alternate" hreflang="en" href="https://example.com/en/post/123" />
     </url>
     ```
   - ক্যানোনিকাল ট্যাগ এবং `hreflang` সিঙ্ক করুন।

5. **টেস্টিং**:
   - Google Search Console-এ সাইটম্যাপ সাবমিট করে ক্রলিং চেক করুন।
   - Robots.txt Tester ব্যবহার করে নিয়ম ভ্যালিডেট করুন।
   - ক্যানোনিকাল ট্যাগ চেক করতে Google-এর URL Inspection Tool ব্যবহার করুন।

6. **ইন্টিগ্রেশন**:
   - Vercel বা Netlify-এর সাথে সাইটম্যাপ এবং Robots.txt স্বয়ংক্রিয় ডিপ্লয় করুন।
   - Next.js-এর `next-sitemap` প্যাকেজ ব্যবহার করে সাইটম্যাপ তৈরি সহজ করুন:
     ```bash
     npm install next-sitemap
     ```
     ```javascript
     // next-sitemap.config.js
     module.exports = {
       siteUrl: "https://example.com",
       generateRobotsTxt: true,
       sitemapSize: 7000,
     };
     ```

7. **সিকিউরিটি**:
   - Robots.txt-এ সংবেদনশীল পেজ (যেমন `/api/keys`) ব্লক করুন।
   - ক্যানোনিকাল URLs HTTPS প্রোটোকল ব্যবহার করুন।

---

## সুবিধা এবং চ্যালেঞ্জ

### সুবিধা:
- **SEO উন্নতি**: সাইটম্যাপ সার্চ ইঞ্জিনে পেজ ইনডেক্সিং ত্বরান্বিত করে।
- **ক্রলিং কন্ট্রোল**: Robots.txt দিয়ে অপ্রয়োজনীয় পেজ ব্লক করা।
- **ডুপ্লিকেট কনটেন্ট প্রতিরোধ**: ক্যানোনিকাল URLs দিয়ে SEO উন্নতি।
- **ইউজার এক্সপেরিয়েন্স**: HTML সাইটম্যাপ দিয়ে নেভিগেশন সহজ।

### চ্যালেঞ্জ:
- **জটিলতা**: ডায়নামিক সাইটম্যাপের জন্য API ইন্টিগ্রেশন।
- **ভুল কনফিগারেশন**: Robots.txt-এ ভুল নিয়ম সাইটের ইনডেক্সিং বন্ধ করতে পারে।
- **ক্যানোনিকাল ট্যাগ ম্যানেজমেন্ট**: ডায়নামিক পেজে সঠিক URL সেট করা।

---

## বাস্তব-বিশ্বে ব্যবহারের উদাহরণ

1. **ই-কমার্স**:
   - **সাইটম্যাপ**: প্রতিটি পণ্য এবং ক্যাটাগরি পেজের তালিকা।
   - **Robots.txt**: অ্যাডমিন এবং কার্ট পেজ ব্লক।
   - **ক্যানোনিকাল URLs**: পণ্য পেজের ফিল্টার URL এড়ানো।
2. **ব্লগ**:
   - **সাইটম্যাপ**: প্রতিটি পোস্ট এবং ক্যাটাগরির তালিকা।
   - **Robots.txt**: ড্রাফ্ট পোস্ট বা ট্যাগ পেজ ব্লক।
   - **ক্যানোনিকাল URLs**: পোস্টের মূল URL নির্দেশ।
3. **মাল্টি-লিঙ্গুয়াল ওয়েবসাইট**:
   - **সাইটম্যাপ**: ভাষা-নির্দিষ্ট পেজ তালিকা।
   - **Robots.txt**: ভাষা-নির্দিষ্ট পেজ ক্রলিং নিয়ন্ত্রণ।
   - **ক্যানোনিকাল URLs**: ভাষা-নির্দিষ্ট ক্যানোনিকাল ট্যাগ।
4. **এন্টারপ্রাইজ অ্যাপ**:
   - **সাইটম্যাপ**: পাবলিক এবং ড্যাশবোর্ড পেজ তালিকা।
   - **Robots.txt**: প্রাইভেট ড্যাশবোর্ড ব্লক।
   - **ক্যানোনিকাল URLs**: ডায়নামিক ড্যাশবোর্ডে ক্যানোনিকাল ট্যাগ।

---

## উপসংহার

Next.js-এ সাইটম্যাপ, Robots.txt, এবং ক্যানোনিকাল URLs ব্যবহার করে ওয়েবসাইটের SEO এবং ক্রলিং প্রক্রিয়া উন্নত করা যায়। সাইটম্যাপ সার্চ ইঞ্জিনকে পেজ ইনডেক্সিংয়ে সহায়তা করে, Robots.txt ক্রলিং নিয়ন্ত্রণ করে, এবং ক্যানোনিকাল URLs ডুপ্লিকেট কনটেন্ট এড়ায়। সিনিয়র ডেভেলপার হিসেবে ডায়নামিক সাইটম্যাপ তৈরি, Robots.txt-এ সঠিক নিয়ম, এবং ক্যানোনিকাল ট্যাগের সঠিক ব্যবহারের দিকে মনোযোগ দিন। Google Search Console এবং টেস্টিং টুল ব্যবহার করে এই উপাদানগুলো ভ্যালিডেট করুন এবং ক্যাশিংয়ের মাধ্যমে পারফরম্যান্স অপটিমাইজ করুন। এর ফলে আপনি একটি SEO-ফ্রেন্ডলি, স্কেলেবল, এবং ইউজার-ফ্রেন্ডলি Next.js অ্যাপ তৈরি করতে পারবেন।